{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"In Colab:\", IN_COLAB)\n",
        "\n",
        "%ls\n",
        "if IN_COLAB:\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    %cd /content/drive/MyDrive/StockClassification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCZdLERjqt1C",
        "outputId": "8024e29d-1da4-4a67-f3bd-16e5b2960823"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Colab: True\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/drive/MyDrive/StockClassification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nLK_0yiMQXau"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import preprecessingData, calculatePriceBB, calculate_yellow_box, calculateTamountBB, makeLabel, sliding_window, EarlyStopping, adjust_learning_rate, setup_logger, makeDataset\n",
        "from model.dlinear import Model as DlinearModel\n",
        "from model.timesnet import Model as TimesnetModel\n",
        "from model.non_transformer import Model as NontransformerModel\n",
        "from time_feature import time_features\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sm5VVv2BYcqW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BEsXn4_TUPp7"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    # torch.use_deterministic_algorithms(True)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Set the seed\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHmnl3LCUhes"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p-kkxNWRRI-2"
      },
      "outputs": [],
      "source": [
        "def makeDataloader(data_X, data_y, BATCH_SIZE=32):\n",
        "  # Tensor로 변환\n",
        "  X_tensor = torch.tensor(data_X, dtype=torch.float32)\n",
        "  y_tensor = torch.tensor(data_y, dtype=torch.float32)\n",
        "\n",
        "  print(f'X_tensor shape : {X_tensor.shape}')\n",
        "  print(f'y_tensor shape : {y_tensor.shape}')\n",
        "\n",
        "\n",
        "  # 학습 및 검증 데이터로 분할 (80% Train, 20% Validation)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2)\n",
        "\n",
        "  # DataLoader 생성\n",
        "  train_dataset = TensorDataset(X_train, y_train)\n",
        "  val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1r5OEWFghdzr"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(outputs, labels):\n",
        "    # 이진 분류의 경우, 시그모이드 함수를 적용한 후 0.5를 기준으로 양/음성 클래스를 결정\n",
        "    preds = torch.round(torch.sigmoid(outputs))\n",
        "    correct = (preds == labels).float().sum()  # 맞춘 예측의 개수\n",
        "    accuracy = correct / labels.numel()  # 전체 데이터 중 맞춘 개수의 비율\n",
        "    return accuracy.item()\n",
        "\n",
        "def calculate_accuracy_for_positives(outputs, labels):\n",
        "    # 이진 분류의 경우, 시그모이드 함수를 적용한 후 0.5를 기준으로 양/음성 클래스를 결정\n",
        "    preds = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "    # 정답이 1인 데이터만 선택\n",
        "    positive_mask = labels == 1\n",
        "\n",
        "    # 해당하는 예측값과 실제값 비교\n",
        "    correct = (preds[positive_mask] == labels[positive_mask]).float().sum()  # 맞춘 예측의 개수\n",
        "    total_positives = positive_mask.float().sum()  # 정답이 1인 데이터의 총 개수\n",
        "\n",
        "    # 1인 데이터가 존재하는 경우에만 계산, 그렇지 않으면 0으로 처리\n",
        "    if total_positives == 0:\n",
        "        return 0.0\n",
        "\n",
        "    accuracy = correct / total_positives  # 1인 데이터 중 맞춘 개수의 비율\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r_rR_70bOT-x"
      },
      "outputs": [],
      "source": [
        "## Focal Loss 함수\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha  # Class balancing factor\n",
        "        self.gamma = gamma  # Focusing parameter\n",
        "        self.reduction = reduction\n",
        "        self.ce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = self.ce_loss(inputs, targets)  # Binary Cross-Entropy loss\n",
        "        pt = torch.exp(-BCE_loss)  # p_t is the probability of the correct class\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return torch.mean(F_loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            return torch.sum(F_loss)\n",
        "        else:\n",
        "            return F_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z6anlfUoRKfN"
      },
      "outputs": [],
      "source": [
        "## 모델 선언\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, drop_out=0.5):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## x: [batch_size, sequence_len, feature_len]\n",
        "        x = self.fc1(x.permute(0,2,1)).permute(0,2,1)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc2(x.permute(0,2,1)).permute(0,2,1)\n",
        "        out = out[:,:,1]\n",
        "\n",
        "        return out\n",
        "\n",
        "class LinearModelFlatten(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, drop_out=0.5):\n",
        "        super(LinearModelFlatten, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## x: [batch_size, sequence_len, feature_len]\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)  # Flatten the input\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        out = self.fc2(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, drop_out=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=drop_out)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, x_mark_enc):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # 마지막 시점의 출력을 사용\n",
        "        return out\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n06NxYNrRLik"
      },
      "outputs": [],
      "source": [
        "def train_model(args, epochs=10):\n",
        "\n",
        "    data_X, data_y = makeDataset(LOAD=True, target=\"신고가\",\n",
        "                                target_feature=args.target_feature,\n",
        "                                target_time=args.target_time)\n",
        "    train_loader, val_loader = makeDataloader(data_X, data_y, BATCH_SIZE=32)\n",
        "\n",
        "    ## 모델 선정\n",
        "    # 모델 초기화\n",
        "    input_size = data_X.shape[2] - 4  # 피처 개수\n",
        "    args.enc_in = input_size\n",
        "    args.dec_in = input_size\n",
        "    hidden_size = 64\n",
        "    output_size = 1\n",
        "    num_layers = 2\n",
        "    drop_out = 0.3\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    model = LSTMModel(input_size, hidden_size, output_size, num_layers, drop_out=drop_out)\n",
        "    if args.model_name == 'LSTM':\n",
        "      model = LSTMModel(input_size, hidden_size, output_size, num_layers, drop_out=drop_out)\n",
        "    elif args.model_name == 'Linear':\n",
        "      model = LinearModel(data_X.shape[1], hidden_size, output_size, drop_out=drop_out)\n",
        "    elif args.model_name == 'LinearFlatten':\n",
        "      model = LinearModelFlatten(data_X.shape[1]*data_X.shape[2], hidden_size, output_size, drop_out=drop_out)\n",
        "    elif args.model_name == 'Dlinear-S':\n",
        "      model = DlinearModel(args, individual=False)\n",
        "    elif args.model_name == 'Dlinear-I':\n",
        "      model = DlinearModel(args, individual=True)\n",
        "    elif args.model_name == 'NontransformerModel':\n",
        "      model = NontransformerModel(args)\n",
        "    elif args.model_name == 'TimesnetModel':\n",
        "      model = TimesnetModel(args)\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    ## 손실 함수 설정\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    if args.loss_func == 'BCE':\n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "    elif args.loss_func == 'Focal':\n",
        "      criterion = FocalLoss()\n",
        "    elif args.loss_func == 'MSE':\n",
        "      criterion = nn.MSELoss()\n",
        "    elif args.loss_func == 'MAE':\n",
        "      criterion = nn.L1Loss()\n",
        "\n",
        "\n",
        "    ## 옵티마이저 및 스케줄러 세팅\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer = optimizer,\n",
        "    #                                         steps_per_epoch = len(train_loader),\n",
        "    #                                         pct_start = 0.3,\n",
        "    #                                         epochs = epochs,\n",
        "    #                                         max_lr = 0.001)\n",
        "    early_stopping = EarlyStopping(patience=20, verbose=True)\n",
        "\n",
        "\n",
        "    ## 로그파일 설정\n",
        "    setting_str = f'{target_time}_{args.model_name}_{args.loss_func}_{epochs}epochs_{\"_\".join(target_feature)}'\n",
        "    logger = setup_logger(log_file=f'{setting_str}_log.txt')\n",
        "\n",
        "\n",
        "\n",
        "    ## 학습 시작 ==============================================\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    accuracy_list = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(f'Epoch: {epoch + 1}/{epochs}')\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            x_mark_enc = X_batch[:, :, -4:]\n",
        "            X_batch, y_batch = X_batch[:, :, :input_size].to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch, x_mark_enc)\n",
        "            loss = criterion(output.squeeze(), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "        logger.info(f'Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        total_val_loss = 0\n",
        "        total_accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                x_mark_enc = X_batch[:, :, -4:]\n",
        "                X_batch, y_batch = X_batch[:, :, :input_size].to(device), y_batch.to(device)\n",
        "                output = model(X_batch, x_mark_enc)\n",
        "                loss = criterion(output.squeeze(), y_batch)\n",
        "                total_val_loss += loss.item()\n",
        "                # 정확도 계산\n",
        "                accuracy = calculate_accuracy_for_positives(output.squeeze(), y_batch)\n",
        "                total_accuracy += accuracy\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        avg_accuracy = total_accuracy / len(val_loader)\n",
        "        accuracy_list.append(avg_accuracy)\n",
        "\n",
        "        valid_loss_list.append(avg_val_loss)\n",
        "\n",
        "        # Early Stopping\n",
        "        early_stopping(-avg_accuracy, model, f'./checkpoints/{setting_str}_')\n",
        "        # 학습률 스케줄러\n",
        "        # scheduler.step(avg_val_loss)\n",
        "\n",
        "        logger.info(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {avg_accuracy:.4f}')\n",
        "\n",
        "\n",
        "    ## 학습 결과(정확도) 기록\n",
        "    last_10_accuracy = sum(accuracy_list[-10:]) / len(accuracy_list[-10:]) if len(accuracy_list) >= 10 else sum(accuracy_list) / len(accuracy_list)\n",
        "    final_accuracy = accuracy_list[-1]\n",
        "    best_accuracy = max(accuracy_list)\n",
        "    logger.info(\"\\n--- Summary ---\")\n",
        "    logger.info(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
        "    logger.info(f\"Last 10 Epochs Average Accuracy: {last_10_accuracy:.4f}\")\n",
        "    logger.info(f\"Final Epoch Accuracy: {final_accuracy:.4f}\")\n",
        "    for handler in logger.handlers:\n",
        "        handler.close()\n",
        "        logger.removeHandler(handler)\n",
        "\n",
        "    return train_loss_list, valid_loss_list, accuracy_list, setting_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args():\n",
        "    def __init__(self, model_name, loss_func, target_time, target_feature):\n",
        "        self.model_name = model_name\n",
        "        self.loss_func = loss_func\n",
        "        self.target_time = target_time\n",
        "        self.target_feature = target_feature\n",
        "\n",
        "        self.task_name = 'classification'\n",
        "        self.seq_len = 10\n",
        "        self.pred_len = 0\n",
        "        self.label_len = 1\n",
        "        self.moving_avg = 3\n",
        "        self.num_class = 1\n",
        "        # 예측할 피처 수\n",
        "        self.c_out = 13\n",
        "\n",
        "        ## 임베딩 관련 인자\n",
        "        # 인코더, 디코더 입력 피처 수\n",
        "        self.enc_in = 13\n",
        "        self.dec_in = 13\n",
        "        # 임베딩 및 레이어의 은닉 상태 크기\n",
        "        self.d_model = 4\n",
        "        # 임베딩 유형\n",
        "        self.embed = 'fixed'\n",
        "        # 입력 데이터 주기\n",
        "        self.freq = 't'\n",
        "        self.dropout = 0.3\n",
        "\n",
        "        ## 어텐션, 인코더/디코더\n",
        "        self.factor = 1\n",
        "        self.output_attention = False\n",
        "        # 어텐션 헤드 수\n",
        "        self.n_heads = 4\n",
        "        # 인코더, 디코더 레이어 수\n",
        "        self.e_layers = 3\n",
        "        self.d_layers = 1\n",
        "        # 피드포워드 네트워크 차원. 주로 d_modl의 4배\n",
        "        self.d_ff = 4 * self.n_heads\n",
        "        # 활성화 함수. relu 혹은 gelu\n",
        "        self.activation = 'gelu'\n",
        "\n",
        "        # Projector 히든 레이어 차원 및 레이어 수\n",
        "        self.p_hidden_dims = [128, 128]\n",
        "        self.p_hidden_layers = 2\n",
        "\n",
        "        self.top_k = 5\n",
        "        self.num_kernels = 6\n",
        "\n",
        "        self.time_feature = True"
      ],
      "metadata": {
        "id": "NTSdmbjT8Ehm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['종가', '거래대금', '거래량볼밴', '주가볼밴', '이등분선']\n",
        "target_feature_list = [['종가', '거래대금', '거래량볼밴', '주가볼밴', '이등분선'],\n",
        "                       ['종가', '거래대금', '거래량볼밴', '이등분선']]\n",
        "# [\"all\", \"AM\"]\n",
        "target_time_list = [\"all\", \"AM\"]\n",
        "# ['BCE', 'Focal']\n",
        "loss_func_list = ['BCE', 'Focal']\n",
        "# ['LSTM', 'Dlinear-S', 'Dlinear-I', 'NontransformerModel', 'TimesnetModel']\n",
        "model_list = ['NontransformerModel', 'TimesnetModel']\n",
        "\n",
        "\n",
        "for target_feature in target_feature_list:\n",
        "  for target_time in target_time_list:\n",
        "    for loss_func in loss_func_list:\n",
        "      for model_name in model_list:\n",
        "\n",
        "        args = Args(model_name, loss_func, target_time, target_feature)\n",
        "\n",
        "        seed_everything(42)\n",
        "\n",
        "        # 모델 학습\n",
        "        print(f'target_feature : {target_feature}')\n",
        "        print(f'target_time : {target_time}')\n",
        "        print(f'loss_func : {loss_func}')\n",
        "        print(f'model_name : {model_name}')\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        train_loss_list, valid_loss_list, accuracy_list, setting_str = train_model(args, epochs=100)\n",
        "\n",
        "        plt.figure(figsize=(16, 10), dpi=300)\n",
        "        plt.plot(train_loss_list, label='Train Loss')\n",
        "        plt.plot(valid_loss_list, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.legend()\n",
        "        plt.savefig(f'./image/{setting_str}_train_loss.png', format='png')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(16, 10), dpi=300)\n",
        "        plt.plot(accuracy_list, label='Valid Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True, axis='y')\n",
        "        plt.legend()\n",
        "        plt.savefig(f'./image/{setting_str}_train_accuracy.png', format='png')\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "cGu5QR9k9Ns1",
        "outputId": "0df506df-b3fb-4487-faf3-e19e95ec237b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target_feature : ['종가', '거래대금', '거래량볼밴', '주가볼밴', '이등분선']\n",
            "target_time : all\n",
            "loss_func : BCE\n",
            "model_name : NontransformerModel\n",
            "==================================================\n",
            "X_tensor shape : torch.Size([166311, 10, 17])\n",
            "y_tensor shape : torch.Size([166311])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-11-05 09:36:10,733 - INFO - ==================================================\n",
            "INFO:train_logger:==================================================\n",
            "2024-11-05 09:36:10,744 - INFO - Epoch: 1/100\n",
            "INFO:train_logger:Epoch: 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-dc31a5710397>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetting_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b7bc282337a7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(args, epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mark_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/StockClassification/model/non_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_enc, x_mark_enc, mask)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mark_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mdec_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mark_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdec_out\u001b[0m  \u001b[0;31m# [B, L, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/StockClassification/model/non_transformer.py\u001b[0m in \u001b[0;36mclassification\u001b[0;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m# Output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the output transformer encoder/decoder embeddings don't include non-linearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_feature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}